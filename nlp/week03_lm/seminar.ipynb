{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"seminar.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mm5fv5qiL0Y9"},"source":["### N-gram language models or how to write scientific papers (4 pts)\n","\n","We shall train our language model on a corpora of [ArXiv](http://arxiv.org/) articles and see if we can generate a new one!\n","\n","![img](https://media.npr.org/assets/img/2013/12/10/istock-18586699-monkey-computer_brick-16e5064d3378a14e0e4c2da08857efe03c04695e-s800-c85.jpg)\n","\n","_data by neelshah18 from [here](https://www.kaggle.com/neelshah18/arxivdataset/)_\n","\n","_Disclaimer: this has nothing to do with actual science. But it's fun, so who cares?!_"]},{"cell_type":"code","metadata":{"id":"XgJ9PZu_L0ZC"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":687},"id":"O-67mHyIL0ZD","executionInfo":{"status":"ok","timestamp":1633423499692,"user_tz":-180,"elapsed":2885,"user":{"displayName":"Екатерина Михайловна Кузина","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10055985327664104868"}},"outputId":"ec655b8d-2551-4fdd-d785-8fc27dfc408f"},"source":["# Alternative manual download link: https://yadi.sk/d/_nGyU2IajjR9-w\n","!wget \"https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\" -O arxivData.json.tar.gz\n","!tar -xvzf arxivData.json.tar.gz\n","data = pd.read_json(\"./arxivData.json\")\n","data.sample(n=5)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-10-05 08:44:56--  https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/dl/99az9n1b57qkd9j/arxivData.json.tar.gz [following]\n","--2021-10-05 08:44:56--  https://www.dropbox.com/s/dl/99az9n1b57qkd9j/arxivData.json.tar.gz\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://ucbac75521e62e0a7129aa0d1074.dl.dropboxusercontent.com/cd/0/get/BXcDS7cZgJE1Fa19YBITkFJCQGDwvGfjlgw8fBGOH0_OCVLheH_RISetsh2hOWmMQEmC9JshdfKCvScVGdIis1I4iKS_2jE1jWI7hLsvyDerd1k2PWvfsnhfjRZoNbmLdWX-57tIejujS6aQf7m5kzqR/file?dl=1# [following]\n","--2021-10-05 08:44:57--  https://ucbac75521e62e0a7129aa0d1074.dl.dropboxusercontent.com/cd/0/get/BXcDS7cZgJE1Fa19YBITkFJCQGDwvGfjlgw8fBGOH0_OCVLheH_RISetsh2hOWmMQEmC9JshdfKCvScVGdIis1I4iKS_2jE1jWI7hLsvyDerd1k2PWvfsnhfjRZoNbmLdWX-57tIejujS6aQf7m5kzqR/file?dl=1\n","Resolving ucbac75521e62e0a7129aa0d1074.dl.dropboxusercontent.com (ucbac75521e62e0a7129aa0d1074.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n","Connecting to ucbac75521e62e0a7129aa0d1074.dl.dropboxusercontent.com (ucbac75521e62e0a7129aa0d1074.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 18933283 (18M) [application/binary]\n","Saving to: ‘arxivData.json.tar.gz’\n","\n","arxivData.json.tar. 100%[===================>]  18.06M  37.2MB/s    in 0.5s    \n","\n","2021-10-05 08:44:58 (37.2 MB/s) - ‘arxivData.json.tar.gz’ saved [18933283/18933283]\n","\n","arxivData.json\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>author</th>\n","      <th>day</th>\n","      <th>id</th>\n","      <th>link</th>\n","      <th>month</th>\n","      <th>summary</th>\n","      <th>tag</th>\n","      <th>title</th>\n","      <th>year</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>39261</th>\n","      <td>[{'name': 'Harsh Agrawal'}, {'name': 'Clint So...</td>\n","      <td>12</td>\n","      <td>1506.04130v3</td>\n","      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n","      <td>6</td>\n","      <td>We are witnessing a proliferation of massive v...</td>\n","      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n","      <td>CloudCV: Large Scale Distributed Computer Visi...</td>\n","      <td>2015</td>\n","    </tr>\n","    <tr>\n","      <th>36104</th>\n","      <td>[{'name': 'Alexey Redozubov'}]</td>\n","      <td>26</td>\n","      <td>1406.6901v2</td>\n","      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n","      <td>6</td>\n","      <td>The structure of the axon-dendrite connections...</td>\n","      <td>[{'term': 'cs.AI', 'scheme': 'http://arxiv.org...</td>\n","      <td>Pattern-wave model of brain. Mechanisms of inf...</td>\n","      <td>2014</td>\n","    </tr>\n","    <tr>\n","      <th>9200</th>\n","      <td>[{'name': 'Yueming Sun'}, {'name': 'Yi Zhang'}...</td>\n","      <td>22</td>\n","      <td>1610.01546v1</td>\n","      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n","      <td>9</td>\n","      <td>We will demonstrate a conversational products ...</td>\n","      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n","      <td>Conversational Recommendation System with Unsu...</td>\n","      <td>2016</td>\n","    </tr>\n","    <tr>\n","      <th>15436</th>\n","      <td>[{'name': 'Miao Fan'}, {'name': 'Kai Cao'}, {'...</td>\n","      <td>7</td>\n","      <td>1504.01683v4</td>\n","      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n","      <td>4</td>\n","      <td>This paper contributes a joint embedding model...</td>\n","      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n","      <td>Jointly Embedding Relations and Mentions for K...</td>\n","      <td>2015</td>\n","    </tr>\n","    <tr>\n","      <th>204</th>\n","      <td>[{'name': 'Çağlar Gülçehre'}, {'name': 'Yoshua...</td>\n","      <td>17</td>\n","      <td>1301.4083v6</td>\n","      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n","      <td>1</td>\n","      <td>We explore the effect of introducing prior inf...</td>\n","      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n","      <td>Knowledge Matters: Importance of Prior Informa...</td>\n","      <td>2013</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                  author  ...  year\n","39261  [{'name': 'Harsh Agrawal'}, {'name': 'Clint So...  ...  2015\n","36104                     [{'name': 'Alexey Redozubov'}]  ...  2014\n","9200   [{'name': 'Yueming Sun'}, {'name': 'Yi Zhang'}...  ...  2016\n","15436  [{'name': 'Miao Fan'}, {'name': 'Kai Cao'}, {'...  ...  2015\n","204    [{'name': 'Çağlar Gülçehre'}, {'name': 'Yoshua...  ...  2013\n","\n","[5 rows x 9 columns]"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MUUXERlQL0ZE","executionInfo":{"status":"ok","timestamp":1633423504239,"user_tz":-180,"elapsed":1861,"user":{"displayName":"Екатерина Михайловна Кузина","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10055985327664104868"}},"outputId":"425fbd41-783c-4f72-dc0e-468a71707306"},"source":["# assemble lines: concatenate title and description\n","lines = data.apply(lambda row: row['title'] + ' ; ' + row['summary'], axis=1).tolist()\n","\n","sorted(lines, key=len)[:3]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Differential Contrastive Divergence ; This paper has been retracted.',\n"," 'What Does Artificial Life Tell Us About Death? ; Short philosophical essay',\n"," 'P=NP ; We claim to resolve the P=?NP problem via a formal argument for P=NP.']"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"GuykdeIDL0ZE"},"source":["### Tokenization\n","\n","You know the dril. The data is messy. Go clean the data. Use WordPunctTokenizer or something.\n"]},{"cell_type":"code","metadata":{"id":"hsAApgBkL0ZF"},"source":["# Task: convert lines (in-place) into strings of space-separated tokens. import & use WordPunctTokenizer\n","\n","# <YOUR CODE>\n","from nltk.tokenize import WordPunctTokenizer\n","\n","tokenizer = WordPunctTokenizer()\n","\n","lines = [\" \".join(tokenizer.tokenize(line.lower())) for line in lines]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C-AlmV9qL0ZF"},"source":["assert sorted(lines, key=len)[0] == \\\n","    'differential contrastive divergence ; this paper has been retracted .'\n","assert sorted(lines, key=len)[2] == \\\n","    'p = np ; we claim to resolve the p =? np problem via a formal argument for p = np .'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1olhLUFYL0ZG"},"source":["### N-Gram Language Model (1point)\n","\n","A language model is a probabilistic model that estimates text probability: the joint probability of all tokens $w_t$ in text $X$: $P(X) = P(w_1, \\dots, w_T)$.\n","\n","It can do so by following the chain rule:\n","$$P(w_1, \\dots, w_T) = P(w_1)P(w_2 \\mid w_1)\\dots P(w_T \\mid w_1, \\dots, w_{T-1}).$$ \n","\n","The problem with such approach is that the final term $P(w_T \\mid w_1, \\dots, w_{T-1})$ depends on $n-1$ previous words. This probability is impractical to estimate for long texts, e.g. $T = 1000$.\n","\n","One popular approximation is to assume that next word only depends on a finite amount of previous words:\n","\n","$$P(w_t \\mid w_1, \\dots, w_{t - 1}) = P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1})$$\n","\n","Such model is called __n-gram language model__ where n is a parameter. For example, in 3-gram language model, each word only depends on 2 previous words. \n","\n","$$\n","    P(w_1, \\dots, w_n) = \\prod_t P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1}).\n","$$\n","\n","You can also sometimes see such approximation under the name of _n-th order markov assumption_."]},{"cell_type":"markdown","metadata":{"id":"P9Dl9GNkL0ZH"},"source":["The first stage to building such a model is counting all word occurences given N-1 previous words"]},{"cell_type":"code","metadata":{"id":"-lpQWL64L0ZI"},"source":["from tqdm import tqdm\n","from collections import defaultdict, Counter\n","\n","# special tokens: \n","# - unk represents absent tokens, \n","# - eos is a special token after the end of sequence\n","\n","UNK, EOS = \"_UNK_\", \"_EOS_\"\n","\n","def count_ngrams(lines, n):\n","    \"\"\"\n","    Count how many times each word occured after (n - 1) previous words\n","    :param lines: an iterable of strings with space-separated tokens\n","    :returns: a dictionary { tuple(prefix_tokens): {next_token_1: count_1, next_token_2: count_2}}\n","\n","    When building counts, please consider the following two edge cases\n","    - if prefix is shorter than (n - 1) tokens, it should be padded with UNK. For n=3,\n","      empty prefix: \"\" -> (UNK, UNK)\n","      short prefix: \"the\" -> (UNK, the)\n","      long prefix: \"the new approach\" -> (new, approach)\n","    - you should add a special token, EOS, at the end of each sequence\n","      \"... with deep neural networks .\" -> (..., with, deep, neural, networks, ., EOS)\n","      count the probability of this token just like all others.\n","    \"\"\"\n","    counts = defaultdict(Counter)\n","    # counts[(word1, word2)][word3] = how many times word3 occured after (word1, word2)\n","\n","    # <YOUR CODE>\n","    for line in lines:\n","        words = [UNK] * (n - 1) + line.split() + [EOS]\n","        for idx in range(n - 1, len(words)):\n","            counts[tuple(words[idx - n + 1:idx])][words[idx]] += 1\n","    return counts\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RaVmMSglL0ZI"},"source":["# let's test it\n","dummy_lines = sorted(lines, key=len)[:100]\n","dummy_counts = count_ngrams(dummy_lines, n=3)\n","assert set(map(len, dummy_counts.keys())) == {2}, \"please only count {n-1}-grams\"\n","assert len(dummy_counts[('_UNK_', '_UNK_')]) == 78\n","assert dummy_counts['_UNK_', 'a']['note'] == 3\n","assert dummy_counts['p', '=']['np'] == 2\n","assert dummy_counts['author', '.']['_EOS_'] == 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mUuCjSp1L0ZJ"},"source":["Once we can count N-grams, we can build a probabilistic language model.\n","The simplest way to compute probabilities is in proporiton to counts:\n","\n","$$ P(w_t | prefix) = { Count(prefix, w_t) \\over \\sum_{\\hat w} Count(prefix, \\hat w) } $$"]},{"cell_type":"code","metadata":{"id":"dti5D6DeL0ZJ"},"source":["class NGramLanguageModel:    \n","    def __init__(self, lines, n):\n","        \"\"\" \n","        Train a simple count-based language model: \n","        compute probabilities P(w_t | prefix) given ngram counts\n","        \n","        :param n: computes probability of next token given (n - 1) previous words\n","        :param lines: an iterable of strings with space-separated tokens\n","        \"\"\"\n","        assert n >= 1\n","        self.n = n\n","    \n","        counts = count_ngrams(lines, self.n)\n","        \n","        # compute token proabilities given counts\n","        self.probs = defaultdict(Counter)\n","        # probs[(word1, word2)][word3] = P(word3 | word1, word2)\n","        \n","        # populate self.probs with actual probabilities\n","        # <YOUR CODE>\n","        for prefix in counts.keys():\n","            s = sum(counts[prefix].values())\n","            for word in counts[prefix].keys():\n","                self.probs[prefix][word] = counts[prefix][word] / s\n","\n","            \n","    def get_possible_next_tokens(self, prefix):\n","        \"\"\"\n","        :param prefix: string with space-separated prefix tokens\n","        :returns: a dictionary {token : it's probability} for all tokens with positive probabilities\n","        \"\"\"\n","        prefix = prefix.split()\n","        prefix = prefix[max(0, len(prefix) - self.n + 1):]\n","        prefix = [ UNK ] * (self.n - 1 - len(prefix)) + prefix\n","        return self.probs[tuple(prefix)]\n","    \n","    def get_next_token_prob(self, prefix, next_token):\n","        \"\"\"\n","        :param prefix: string with space-separated prefix tokens\n","        :param next_token: the next token to predict probability for\n","        :returns: P(next_token|prefix) a single number, 0 <= P <= 1\n","        \"\"\"\n","        return self.get_possible_next_tokens(prefix).get(next_token, 0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u0qfxd5cL0ZK"},"source":["Let's test it!"]},{"cell_type":"code","metadata":{"id":"PKAeIPorL0ZK"},"source":["dummy_lm = NGramLanguageModel(dummy_lines, n=3)\n","\n","p_initial = dummy_lm.get_possible_next_tokens('') # '' -> ['_UNK_', '_UNK_']\n","assert np.allclose(p_initial['learning'], 0.02)\n","assert np.allclose(p_initial['a'], 0.13)\n","assert np.allclose(p_initial.get('meow', 0), 0)\n","assert np.allclose(sum(p_initial.values()), 1)\n","\n","p_a = dummy_lm.get_possible_next_tokens('a') # '' -> ['_UNK_', 'a']\n","assert np.allclose(p_a['machine'], 0.15384615)\n","assert np.allclose(p_a['note'], 0.23076923)\n","assert np.allclose(p_a.get('the', 0), 0)\n","assert np.allclose(sum(p_a.values()), 1)\n","\n","assert np.allclose(dummy_lm.get_possible_next_tokens('a note')['on'], 1)\n","assert dummy_lm.get_possible_next_tokens('a machine') == \\\n","    dummy_lm.get_possible_next_tokens(\"there have always been ghosts in a machine\"), \\\n","    \"your 3-gram model should only depend on 2 previous words\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lRHxVKvjL0ZK"},"source":["Now that you've got a working n-gram language model, let's see what sequences it can generate. But first, let's train it on the whole dataset."]},{"cell_type":"code","metadata":{"id":"OuQ7ysMLL0ZL"},"source":["lm = NGramLanguageModel(lines, n=3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZM166LVTL0ZL"},"source":["The process of generating sequences is... well, it's sequential. You maintain a list of tokens and iteratively add next token by sampling with probabilities.\n","\n","$ X = [] $\n","\n","__forever:__\n","* $w_{next} \\sim P(w_{next} | X)$\n","* $X = concat(X, w_{next})$\n","\n","\n","Instead of sampling with probabilities, one can also try always taking most likely token, sampling among top-K most likely tokens or sampling with temperature. In the latter case (temperature), one samples from\n","\n","$$w_{next} \\sim {P(w_{next} | X) ^ {1 / \\tau} \\over \\sum_{\\hat w} P(\\hat w | X) ^ {1 / \\tau}}$$\n","\n","Where $\\tau > 0$ is model temperature. If $\\tau << 1$, more likely tokens will be sampled with even higher probability while less likely tokens will vanish."]},{"cell_type":"code","metadata":{"id":"0cTGlVPJL0ZL"},"source":["def get_next_token(lm, prefix, temperature=1.0):\n","    \"\"\"\n","    return next token after prefix;\n","    :param temperature: samples proportionally to lm probabilities ^ (1 / temperature)\n","        if temperature == 0, always takes most likely token. Break ties arbitrarily.\n","    \"\"\"\n","    # <YOUR CODE>\n","    suffixes = lm.get_possible_next_tokens(prefix)\n","    tokens, token_probs = list(suffixes.keys()), list(suffixes.values())\n","    if temperature == 0.:\n","        return tokens[np.argmax(token_probs)]\n","    token_probs = np.array([p ** (1 / temperature) for p in token_probs])\n","    total = sum(token_probs)\n","    token_probs = token_probs / total\n","    return np.random.choice(tokens, p=token_probs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CMRy_CpbL0ZL","executionInfo":{"status":"ok","timestamp":1633423539529,"user_tz":-180,"elapsed":6418,"user":{"displayName":"Екатерина Михайловна Кузина","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10055985327664104868"}},"outputId":"4b6e8eae-54d0-4030-acf5-288f39f4cd1d"},"source":["from collections import Counter\n","test_freqs = Counter([get_next_token(lm, 'there have') for _ in range(10000)])\n","assert 250 < test_freqs['not'] < 450\n","assert 8500 < test_freqs['been'] < 9500\n","assert 1 < test_freqs['lately'] < 200\n","\n","test_freqs = Counter([get_next_token(lm, 'deep', temperature=1.0) for _ in range(10000)])\n","assert 1500 < test_freqs['learning'] < 3000\n","test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.5) for _ in range(10000)])\n","assert 8000 < test_freqs['learning'] < 9000\n","test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.0) for _ in range(10000)])\n","assert test_freqs['learning'] == 10000\n","\n","print(\"Looks nice!\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looks nice!\n"]}]},{"cell_type":"markdown","metadata":{"id":"o0ujm3YRL0ZM"},"source":["Let's have fun with this model"]},{"cell_type":"code","metadata":{"id":"O4uSIpRiL0ZM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633423539530,"user_tz":-180,"elapsed":16,"user":{"displayName":"Екатерина Михайловна Кузина","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10055985327664104868"}},"outputId":"25109c77-20ca-4c00-d434-9f3ffb9d68cf"},"source":["prefix = 'artificial' # <- your ideas :)\n","\n","for i in range(100):\n","    prefix += ' ' + get_next_token(lm, prefix)\n","    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n","        break\n","        \n","print(prefix)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["artificial neural networks . we improve the controller to program in this situation , i note that regarding new variables , a ) we suggest the exploration of poorly calibrated lens distortion . minimizing the worst case predictive covariance of the solution function of the system . this paper , we consider infinite - order markov chains ( ccs ) some upper bounds ( ucbs ) to illuminate a scene . super - resolution images consists of all channels respectively . our implementation involves spectral image registration as a result , populous can allow for future work and simpler subtasks .\n"]}]},{"cell_type":"code","metadata":{"id":"g2buc1dqL0ZM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633423539531,"user_tz":-180,"elapsed":13,"user":{"displayName":"Екатерина Михайловна Кузина","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10055985327664104868"}},"outputId":"1ad9fdd1-fe89-4b27-8945-d70ed68536ad"},"source":["prefix = 'bridging the' # <- more of your ideas\n","\n","for i in range(100):\n","    prefix += ' ' + get_next_token(lm, prefix, temperature=0.5)\n","    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n","        break\n","        \n","print(prefix)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["bridging the gap between the two images . the proposed method is evaluated on a large number of hidden units in deep learning , where each pixel in the last few years . however , it is shown to improve the performance of the target domain . we show that the proposed model . the algorithm is used to perform . however , it is possible to use a fixed size . we show that the proposed method is a fundamental problem in this paper , we propose to explore the problem of detecting and classifying lesions in ct images ; in\n"]}]},{"cell_type":"markdown","metadata":{"id":"zHi-zIInL0ZM"},"source":["__More in the homework:__ nucleous sampling, top-k sampling, beam search(not for the faint of heart)."]},{"cell_type":"markdown","metadata":{"id":"LArzMBtfL0ZN"},"source":["### Evaluating language models: perplexity (1point)\n","\n","Perplexity is a measure of how well does your model approximate true probability distribution behind data. __Smaller perplexity = better model__.\n","\n","To compute perplexity on one sentence, use:\n","$$\n","    {\\mathbb{P}}(w_1 \\dots w_N) = P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_t P(w_t \\mid w_{t - n}, \\dots, w_{t - 1})\\right)^{-\\frac1N},\n","$$\n","\n","\n","On the corpora level, perplexity is a product of probabilities of all tokens in all sentences to the power of 1, divided by __total length of all sentences__ in corpora.\n","\n","This number can quickly get too small for float32/float64 precision, so we recommend you to first compute log-perplexity (from log-probabilities) and then take the exponent."]},{"cell_type":"code","metadata":{"id":"jvf7v4LWL0ZN"},"source":["def perplexity(lm, lines, min_logprob=np.log(10 ** -50.)):\n","    \"\"\"\n","    :param lines: a list of strings with space-separated tokens\n","    :param min_logprob: if log(P(w | ...)) is smaller than min_logprop, set it equal to min_logrob\n","    :returns: corpora-level perplexity - a single scalar number from the formula above\n","    \n","    Note: do not forget to compute P(w_first | empty) and P(eos | full_sequence)\n","    \n","    PLEASE USE lm.get_next_token_prob and NOT lm.get_possible_next_tokens\n","    \"\"\"\n","    # <YOUR CODE>\n","    log_perplexity = 0\n","    n = 0\n","    for line in lines:\n","        words = line.split() + [EOS]\n","        n += len(words)\n","        for idx, word in enumerate(words):\n","            prefix = \" \".join(words[:idx])\n","            prob = lm.get_next_token_prob(prefix, word)\n","            log_prob = min_logprob\n","            if prob > 0:\n","                log_prob = max(np.log(prob), min_logprob)\n","            log_perplexity += log_prob\n","    return np.exp(-log_perplexity / n)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wsxdcyc_L0ZN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633423540019,"user_tz":-180,"elapsed":497,"user":{"displayName":"Екатерина Михайловна Кузина","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10055985327664104868"}},"outputId":"ea600169-9db6-4a19-fd5c-46a159addd2a"},"source":["lm1 = NGramLanguageModel(dummy_lines, n=1)\n","lm3 = NGramLanguageModel(dummy_lines, n=3)\n","lm10 = NGramLanguageModel(dummy_lines, n=10)\n","\n","ppx1 = perplexity(lm1, dummy_lines)\n","ppx3 = perplexity(lm3, dummy_lines)\n","ppx10 = perplexity(lm10, dummy_lines)\n","ppx_missing = perplexity(lm3, ['the jabberwock , with eyes of flame , '])  # thanks, L. Carrol\n","\n","print(\"Perplexities: ppx1=%.3f ppx3=%.3f ppx10=%.3f\" % (ppx1, ppx3, ppx10))\n","\n","assert all(0 < ppx < 500 for ppx in (ppx1, ppx3, ppx10)), \"perplexity should be nonnegative and reasonably small\"\n","assert ppx1 > ppx3 > ppx10, \"higher N models should overfit and \"\n","assert np.isfinite(ppx_missing) and ppx_missing > 10 ** 6, \"missing words should have large but finite perplexity. \" \\\n","    \" Make sure you use min_logprob right\"\n","assert np.allclose([ppx1, ppx3, ppx10], (318.2132342216302, 1.5199996213739575, 1.1838145037901249))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Perplexities: ppx1=318.213 ppx3=1.520 ppx10=1.184\n"]}]},{"cell_type":"markdown","metadata":{"id":"yO96U991L0ZO"},"source":["Now let's measure the actual perplexity: we'll split the data into train and test and score model on test data only."]},{"cell_type":"code","metadata":{"id":"aS3wbuOhL0ZO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633423631284,"user_tz":-180,"elapsed":91268,"user":{"displayName":"Екатерина Михайловна Кузина","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10055985327664104868"}},"outputId":"d598cf82-de24-41d0-9868-ce9c17746476"},"source":["from sklearn.model_selection import train_test_split\n","train_lines, test_lines = train_test_split(lines, test_size=0.25, random_state=42)\n","\n","for n in (1, 2, 3):\n","    lm = NGramLanguageModel(n=n, lines=train_lines)\n","    ppx = perplexity(lm, test_lines)\n","    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["N = 1, Perplexity = 1832.23136\n","N = 2, Perplexity = 85653987.28774\n","N = 3, Perplexity = 61999196259043346743296.00000\n"]}]},{"cell_type":"code","metadata":{"id":"A-J76Q0_L0ZO"},"source":["# whoops, it just blew up :)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0wVchHhdL0ZO"},"source":["### LM Smoothing\n","\n","The problem with our simple language model is that whenever it encounters an n-gram it has never seen before, it assigns it with the probabilitiy of 0. Every time this happens, perplexity explodes.\n","\n","To battle this issue, there's a technique called __smoothing__. The core idea is to modify counts in a way that prevents probabilities from getting too low. The simplest algorithm here is Additive smoothing (aka [Lapace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)):\n","\n","$$ P(w_t | prefix) = { Count(prefix, w_t) + \\delta \\over \\sum_{\\hat w} (Count(prefix, \\hat w) + \\delta) } $$\n","\n","If counts for a given prefix are low, additive smoothing will adjust probabilities to a more uniform distribution. Not that the summation in the denominator goes over _all words in the vocabulary_.\n","\n","Here's an example code we've implemented for you:"]},{"cell_type":"code","metadata":{"id":"6PEt2pLVL0ZP"},"source":["class LaplaceLanguageModel(NGramLanguageModel): \n","    \"\"\" this code is an example, no need to change anything \"\"\"\n","    def __init__(self, lines, n, delta=1.0):\n","        self.n = n\n","        counts = count_ngrams(lines, self.n)\n","        self.vocab = set(token for token_counts in counts.values() for token in token_counts)\n","        self.probs = defaultdict(Counter)\n","\n","        for prefix in counts:\n","            token_counts = counts[prefix]\n","            total_count = sum(token_counts.values()) + delta * len(self.vocab)\n","            self.probs[prefix] = {token: (token_counts[token] + delta) / total_count\n","                                          for token in token_counts}\n","    def get_possible_next_tokens(self, prefix):\n","        token_probs = super().get_possible_next_tokens(prefix)\n","        missing_prob_total = 1.0 - sum(token_probs.values())\n","        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n","        return {token: token_probs.get(token, missing_prob) for token in self.vocab}\n","    \n","    def get_next_token_prob(self, prefix, next_token):\n","        token_probs = super().get_possible_next_tokens(prefix)\n","        if next_token in token_probs:\n","            return token_probs[next_token]\n","        else:\n","            missing_prob_total = 1.0 - sum(token_probs.values())\n","            missing_prob_total = max(0, missing_prob_total) # prevent rounding errors\n","            return missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qseFpTw2L0ZP"},"source":["#test that it's a valid probability model\n","for n in (1, 2, 3):\n","    dummy_lm = LaplaceLanguageModel(dummy_lines, n=n)\n","    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"01msUx41L0ZP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633423745403,"user_tz":-180,"elapsed":114128,"user":{"displayName":"Екатерина Михайловна Кузина","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10055985327664104868"}},"outputId":"c18db3ac-6e7e-4c7c-e437-da5f6bd6b01f"},"source":["for n in (1, 2, 3):\n","    lm = LaplaceLanguageModel(train_lines, n=n, delta=0.1)\n","    ppx = perplexity(lm, test_lines)\n","    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["N = 1, Perplexity = 977.67559\n","N = 2, Perplexity = 470.48021\n","N = 3, Perplexity = 3679.44765\n"]}]},{"cell_type":"code","metadata":{"id":"WNHbltC_L0ZP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633423756006,"user_tz":-180,"elapsed":10613,"user":{"displayName":"Екатерина Михайловна Кузина","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10055985327664104868"}},"outputId":"2ff0fadf-2c79-49c0-bf0c-283b50712b51"},"source":["# optional: try to sample tokens from such a model\n","\n","lm = LaplaceLanguageModel(train_lines, n=2, delta=0.1)\n","prefix = 'machine'\n","for i in range(10):\n","    prefix += ' ' + get_next_token(lm, prefix)\n","    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n","        break\n","        \n","print(prefix)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["machine fiction variationalapproximations wips volcano proof search . _EOS_\n"]}]},{"cell_type":"markdown","metadata":{"id":"GHgfX_W_L0ZP"},"source":["### Kneser-Ney smoothing (2 points)\n","\n","Additive smoothing is simple, reasonably good but definitely not a State of The Art algorithm.\n","\n","\n","Your final task in this notebook is to implement [Kneser-Ney](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing) smoothing.\n","\n","It can be computed recurrently, for n>1:\n","\n","$$P_{kn}(w_t | prefix_{n-1}) = { \\max(0, Count(prefix_{n-1}, w_t) - \\delta) \\over \\sum_{\\hat w} Count(prefix_{n-1}, \\hat w)} + \\lambda_{prefix_{n-1}} \\cdot P_{kn}(w_t | prefix_{n-2})$$\n","\n","where\n","- $prefix_{n-1}$ is a tuple of {n-1} previous tokens\n","- $lambda_{prefix_{n-1}}$ is a normalization constant chosen so that probabilities add up to 1\n","- Unigram $P_{kn}(w_t | prefix_{n-2})$ corresponds to Kneser Ney smoothing for {N-1}-gram language model.\n","- Unigram $P_{kn}(w_t)$ is a special case: how likely it is to see x_t in an unfamiliar context\n","\n","See lecture slides or wiki for more detailed formulae.\n","\n","__Your task__ is to\n","- implement KneserNeyLanguageModel\n","- test it on 1-3 gram language models\n","- find optimal (within reason) smoothing delta for 3-gram language model with Kneser-Ney smoothing"]},{"cell_type":"code","metadata":{"id":"ysDhXxRKL0ZQ"},"source":["class KneserNeyLanguageModel(NGramLanguageModel): \n","    \"\"\" A template for Kneser-Ney language model. Default delta may be suboptimal. \"\"\"\n","    def __init__(self, lines, n, delta=1.0):\n","        self.n = n\n","        # <YOUR CODE>\n","        self.delta = delta\n","        self.probs = defaultdict(Counter)\n","\n","        counts = count_ngrams(lines=lines, n=1)\n","        for prefix in counts.keys():\n","            token_counts = counts[prefix]\n","            total_count = sum(token_counts.values())\n","            self.probs[prefix] = {token: token_counts[token] / total_count for token in token_counts}\n","        self.vocab = set(token for token_counts in counts.values() for token in token_counts)\n","\n","        for i in range(1, n + 1):\n","            counts = count_ngrams(lines=lines, n=i)\n","            for prefix in counts.keys():\n","                token_counts = counts[prefix]\n","                total_count = sum(token_counts.values())\n","                for token in token_counts: \n","                    a = max(0, token_counts[token] - self.delta) / total_count\n","                    b = self.delta * len(token_counts.keys()) * self.probs[tuple(prefix[1:])][token] / total_count\n","                    self.probs[prefix][token] = a + b\n","        \n","    def get_possible_next_tokens(self, prefix):\n","        token_probs = super().get_possible_next_tokens(prefix)\n","        missing_prob_total = 1.0 - sum(token_probs.values())\n","        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n","        return {token: token_probs.get(token, missing_prob) for token in self.vocab}\n","    \n","    def get_next_token_prob(self, prefix, next_token):\n","        token_probs = super().get_possible_next_tokens(prefix)\n","        if next_token in token_probs:\n","            return token_probs[next_token]\n","        else:\n","            missing_prob_total = 1.0 - sum(token_probs.values())\n","            missing_prob_total = max(0, missing_prob_total) # prevent rounding errors\n","            return missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HNhi7_i7L0ZQ"},"source":["#test that it's a valid probability model\n","for n in (1, 2, 3):\n","    dummy_lm = KneserNeyLanguageModel(dummy_lines, n=n)\n","    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T0jsmTFLL0ZQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633425459977,"user_tz":-180,"elapsed":148450,"user":{"displayName":"Екатерина Михайловна Кузина","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10055985327664104868"}},"outputId":"92ad5ce9-cdc5-4024-a5fa-645b760fadfd"},"source":["for n in (1, 2, 3):\n","    lm = KneserNeyLanguageModel(train_lines, n=n, delta=1)\n","    ppx = perplexity(lm, test_lines)\n","    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["N = 1, Perplexity = 1844.53488\n","N = 2, Perplexity = 267.13071\n","N = 3, Perplexity = 966.22856\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vRczBDNesXES","executionInfo":{"status":"ok","timestamp":1633425508122,"user_tz":-180,"elapsed":19386,"user":{"displayName":"Екатерина Михайловна Кузина","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10055985327664104868"}},"outputId":"89e27b31-e311-4245-f87a-9d70670632e2"},"source":["lm = KneserNeyLanguageModel(train_lines, n=2)\n","prefix = 'artificial'\n","for i in range(10):\n","    prefix += ' ' + get_next_token(lm, prefix)\n","    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n","        break\n","        \n","print(prefix)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["artificial intelligence : experimental results show that match ( stl and\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C_OIKVJSv2i5","executionInfo":{"status":"ok","timestamp":1633425556359,"user_tz":-180,"elapsed":1099,"user":{"displayName":"Екатерина Михайловна Кузина","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10055985327664104868"}},"outputId":"15d4cf48-b36b-49e1-9335-fd75c0afc27d"},"source":["prefix = 'artificial'\n","for i in range(10):\n","    prefix += ' ' + get_next_token(lm, prefix)\n","    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n","        break\n","        \n","print(prefix)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["artificial intelligence as a new baseline , and their efficiency ,\n"]}]},{"cell_type":"code","metadata":{"id":"s0vPNG38v-hd"},"source":[""],"execution_count":null,"outputs":[]}]}